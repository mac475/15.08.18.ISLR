{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\", \"malgun gothic\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width: 100%;\n",
       "    }\n",
       "    ul {\n",
       "        line-height: 145%;\n",
       "        font-size: 90%;\n",
       "    }\n",
       "    li {\n",
       "        margin-bottom: 0.2em;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top: 12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width: 90%;\n",
       "//        margin-left:auto;\n",
       "//        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mac475의 ipython 표준 style을 적용함\n",
    "from IPython.core.display import HTML\n",
    "styles = open(\"styles/custom.css\", \"r\").read()\n",
    "HTML( styles )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Qualitative value를 다루는 분야를 classification이라 하며, qualitative/ categorical이라고도 함\n",
    "* 기본적으로는 특정 category에 대한하는 확률 (probability)에 대한 문제이므로, regression과 상이하지는 않음\n",
    "* logistic regression, linear discriminant analysis, K-nearest neighbors를 먼저 study후\n",
    "* generalized additive models, trees, random forests, boosting, support vector machines를 이후에 study함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.1 An Overview of Classfication\n",
    "##4.2 Why Not Linear Regression\n",
    "##4.3 Logistic Regression\n",
    "* logistic regression은 Y값이 특정 category에 포함되는 확률값 : probability를 modeling함\n",
    "* $Pr(default = Yes | balance) = p( balance ) , 0 \\le p( balance ) \\le 1$\n",
    "<img src=\"images/p131 figure 4.2.png\" width = \"65%\"/>\n",
    "* 좌측의 linear regression의 경우 negative값도 존재하나, 우측의 logistic regression의 경우 0~1 값만 가짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.3.1 The Logistic Model\n",
    "* $p(X) = Pr(Y = 1 | X)$에 대한 modeling은 $p(X) = \\beta_0 + \\beta_1X$와 같은 형태로 시작\n",
    "* 이때, regression의 결과가 항상 0~1사이의 값으로 제공되어야 하므로, 적절한 function으로 표현필요\n",
    "* logistic function : $$p(X) = \\frac{e^{\\beta_0 + \\beta_1X} }{ 1 + e^{\\beta_0 + \\beta_1X} }$$\n",
    "* logistic regression의 fit (계수추정)은 maximum likehood (최대 가능 도방법) 활용하여 수행\n",
    "* logistic function의 변환 : $$\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1X}$$\n",
    "* 이때 $p(X) / 1 - p(X)$를 odds, 혹은 odds ratio라 함\n",
    "* ex) $p(X) = 0.2$의 경우 odds = 1/4이고, $p(X) = 0.9$의 경우, odds = 9임\n",
    "* logistic function에 logarithm을 적용하면,\n",
    "$$log(\\frac{p(X)}{1-p(X)}) = \\beta_0 + \\beta_1X$$\n",
    "* 이때 좌측을 logit이라 하며, $\\beta_0 + \\beta_1X$로 표현되는 linear임을 알 수 있다\n",
    "* 또한, 입력 값의 범위가 $[-\\infty,+\\infty]$일 때 출력 값의 범위를 [0,1]로 조정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.3.2 Estimating the Regression Coeffients\n",
    "* 일반적으로 non-linear의 경우, fitting을 위해 maximum likehood (최대 가능 도방법)을 사용\n",
    "* $l(\\beta_0, \\beta_1) = \\prod\\limits_{i:y_i = 1}p(x_i)\\prod\\limits_{i\\prime:y_{i\\prime}=0)}( 1 - p(x_{i\\prime}))$\n",
    "* 잘 이해되지 않음, 하지만 non-linear model에 대한 fitting시 가장 일반적인 방법임\n",
    "* z값 (z-statistic)은 linear regression에서의 t값 (t-statistic)과 같은 의미이며, z값이 클수록 귀무가설 $H_0 : \\beta_1 = 0$을 기각한다. 즉, 이 귀무가설은 $\\beta_1 = 0$을 의미하므로 $$p(X) = \\frac{e^{\\beta_0 + \\beta_1X} }{ 1 + e^{\\beta_0 + \\beta_1X} } = \\frac{e^{\\beta_0}}{1+e^{\\beta_0}}$$가 된다는 의미이다\n",
    "<img src=\"images/p134 table 4.1.png\" width = \"65%\"/>\n",
    "* 이 경우, p-value가 충분히 작으므로, balance와 default의 probability간은 관계가 있다고 보여짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.3.3 Making Predictions\n",
    "<img src=\"images/p135 table 4.2.png\" width = \"65%\"/>\n",
    "* 이때, student[Yes]에 대한 p-value가 충분히 작으므로, 귀무가설 기각하여 학생여부는 default에 유의미함\n",
    "* 학생대상 default 확률 $$\\widehat{Pr}(default = Yes|student = Yes) = \\frac{e^{-3.5041+0.4049 \\times 1}}{1+e^{-3.5041+0.4049 \\times 1}} = 0.0431$$\n",
    "* 일반인대상 default 확률 $$\\widehat{Pr}(default = Yes|student = Yes) = \\frac{e^{-3.5041+0.4049 \\times }}{1+e^{-3.5041+0.4049 \\times 0}} = 0.0292$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.3.4 Multiple Logistic Regression\n",
    "* Multiple predictor 기반의 경우, $$log(\\frac{p(X)}{1-p(X)}) = \\beta_0 + \\beta_1X_1 + \\cdots + \\beta_pX_p$$이고,\n",
    "  $$p(X) = \\frac{e^{\\beta_0 + \\beta_1X_1 + \\cdots + \\beta_pX_p}}{1 + e^{\\beta_0+\\beta_1X_1 + \\cdots + \\beta_pX_p}}$$와 같이 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.3.5 Logistic Regression for > 2 Response Classes\n",
    "* 2개 이상의 class를 다루는 경우 discriminant analysis를 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.4 Linear Discriminant Analysis\n",
    "* Logistic Regression은 직접적으로 $Pr(Y = k | X = x)$를 logistic function을 이용하여 구하는 방법\n",
    "* 대안으로써 각각의 predictor X에 대해, response class Y별 distribution을 modeling 가능하며, 그 이후 Bayes' 정리를 활용하여 $Pr(Y = k | X = x)$를 구할 수 있음 (베이즈의 공식)\n",
    "* 참고 : Bayes'의 공식\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "####Logistic Regression 대신 LDA를 사용하는 이유\n",
    "1. class가 잘 분리된 경우, logistic regression의 parameter estimation의 불안정하나 LDA는 그렇지 않음\n",
    "2. $n$이 매우 작고, X가 각 class에 대해 normal distribution 경우, LDA가 logistic regression 비해 안정적\n",
    "3. response class가 2개 이상인 경우, LDA가 더 유용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.4.1 Using Bayes' Therem for Classification\n",
    "* $\\pi_k$ : overall/ prior probability : 무작위 추출된 observation이 $k$ class인 probability\n",
    "* X의 density function $f_k(X) \\equiv Pr(X = x|Y = k)$ : X가 $k$ class일 density function\n",
    "* Bayes' therem : $$Pr( Y = k | X = x ) = \\frac{\\pi_kf_k(X)}{\\sum\\limits_{l = 1}^K\\pi_lf_l(x)}$$\n",
    "\n",
    "* 일반적으로 $\\pi_k$는 계산이 쉬우나, $f_k(X)$는 계산이 어려움\n",
    "* $p_k(X) = Pr(Y=k | X)$를 우리는 posterior probability라 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.4.2 Linear Discriminant Analysis for p = 1\n",
    "* p = 1, 하나의 predictor만을 가정시, Bayes's 정리에 대응을 위한 $f_k(x)$ 확보를 통해, $p_k(x)$를 estimation하고\n",
    "* 이때, $p_k(x)$가 최고치인 것으로 observation을 classify함\n",
    "* 참고 : 정규분포 (Normal distribution)\n",
    "  $$f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\frac{-(x-m)^2}{2\\sigma^2}}$$\n",
    "  \n",
    "* 따라서, $f_k(x)$를 normal, Gaussian으로 가정한다면 다음과 같이 표현가능하며,\n",
    "  $$f_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_k}exp\\left(-\\frac{1}{2\\sigma_k^2}(x - \\mu_k)^2\\right)$$\n",
    "    - 이때, $\\mu_k$는 $k$ class의 평균, $\\sigma^2$은 $k$ class의 variance (분산)임\n",
    "    - 편의상, $\\sigma_1^2 = \\sigma_2^2 = \\cdots = \\sigma_K^2$으로 가정하고, $K$ classes들에 대해 통용되는 variance로 가정하면,\n",
    "    - $f_k(x)$를 Bayes' 정리에 대입시, 다음과 같이 정리됨\n",
    "    $$p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{1}{2\\sigma^2}(x-\\mu_k)^2)}{\\sum\\limits_{l=1}^K\\pi_l\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{1}{2\\sigma^2}(x-\\mu_l)^2)}$$\n",
    "    - log를 취하고 공식정리하면,\n",
    "      $$\\delta_k(x) = x \\cdot  \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + log(\\pi_k)$$이 되고, $K=2, \\pi_1 = \\pi_2$로 가정시, Bayes' classifier는\n",
    "    $$\\delta_1(x) - \\delta_2(x) = \\left( x \\cdot  \\frac{\\mu_1}{\\sigma^2} - \\frac{\\mu_1^2}{2\\sigma^2} + log(\\pi_1) \\right) - \\left(x \\cdot  \\frac{\\mu_2}{\\sigma^2} - \\frac{\\mu_2^2}{2\\sigma^2} + log(\\pi_2)\\right)$$와 같이 정리되며, 따라서\n",
    "    $$2x(\\mu_1 - \\mu_2) - (\\mu^2 + \\mu^2)$$으로 정리된다. 따라서, classifier는\n",
    "    $$2x(\\mu_1 - \\mu_2) > (\\mu^2 + \\mu^2)$$일때 $K = 1$, 아닌경우, $K = 2$로 배정한다\n",
    "    - 따라서, 이러한 경우 Bayes decision boundary는 다음과 같다\n",
    "    $$x = \\frac{\\mu_1^2 - \\mu_2^2}{2(\\mu_1 - \\mu_2)} = \\frac{\\mu_1 + \\mu_2}{2}$$\n",
    "    - estimate에 대한 LDA classifier는\n",
    "    $$\\hat{\\delta_k}(x) = x \\cdot  \\frac{\\hat{\\mu_k}}{\\hat{\\sigma}^2} - \\frac{\\hat{\\mu_k}^2}{2\\hat{\\sigma}^2} + log(\\hat{\\pi}_k)$$와 같이 표현되며, 이때 $linear$ 표현은 discriminant function $\\hat{\\delta_k}(x)$가 $x$에 대해 $linear$한 것에서 기인함\n",
    "\n",
    "* 다소 이상적 가정에 기반하였으며, 4.4.4에서 $k$th class가 $\\sigma_k^2$를 가지는 좀 더 설득력 있는 상황을 study함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.4.3 Linear Discriminant Analysis for p > 1\n",
    "* LDA classifier를 multiple predictor까지 확대\n",
    "* $X = (X_1, X2, \\cdots, X_p)$가 multivariate Gaussian (다변량 가우시안), 혹은 multivariate normal (다변량 정규) distribution에서 추출된다 가정 (이때, class specific mean vector와 common covariance matrix를 가짐)\n",
    "* multivariate Gaussian distribution은 각각의 predictor들이 normal distribution이라 가정 (단, predictor간 correlation 존재가능)\n",
    "* p-dimensional random variable X가 multivariate Gaussian distribution일때 다음과 같이 표기\n",
    "  \n",
    "  $X \\sim N(\\mu, \\Sigma)$ 이때, $E(X) = \\mu$는 X의 mean값이고, $Cov(X) = \\Sigma$는 $p \\times p$는 X의 covariance matrix임\n",
    "\n",
    "* multivariate Gaussian density function은 다음과 같이 표현\n",
    "  $$f(x) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)$$\n",
    "  되며, 이것을 다음의 Bayes' theorem에 대입/ 정리하면,\n",
    "  $$Pr( Y = k | X = x ) = \\frac{\\pi_kf_k(X)}{\\sum\\limits_{l = 1}^K\\pi_lf_l(x)}$$\n",
    "  \n",
    "  최종적으로, Bayes classifier는 $X = x$에 대해서 다음의 $\\delta_k(x)$가 최대인 class에 assign한다\n",
    "  $$\\delta_k(x) = x^T\\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + log\\pi_k$$\n",
    "  이때, $\\delta_k(x)$가 $x$에 대한 linear임을 주목\n",
    "\n",
    "\n",
    "* 일반적으로 training error rate은 test error rate보다 낮음\n",
    "* sample의 개수 $n$에 대한 parameter의 개수 $p$의 비율이 높아질수록 overfitting 가능성 높음\n",
    "\n",
    "\n",
    "* Class specific performance 측정시 다음과 같은 confusion matrix 활용\n",
    "\n",
    "<img src=\"images/p145 table 4.4.png\" width = \"65%\"/>\n",
    "* sensitivity : 실제 있는 것을 있다고 식별하는 확률\n",
    "* specificity : 실제 없는 것을 없다고 식별하는 확률\n",
    "<img src=\"images/p145 table 4.x.png\" width = \"65%\"/>\n",
    "\n",
    "* 다음을 보면, 신용카드 회사의 시각에서 실제 default = Yes를 predict 해낸 performance = 24.3%임\n",
    "<img src=\"images/p145 table 4.y.png\" width = \"65%\"/>\n",
    "    - 하지만, 신용카드 회사의 필요성 관점으로 보면, classifier의 performance를 향상시켜야 하며 이는 다음과 같이 threshold 하향조정 통해 수행\n",
    "    $$Pr(default = Yes | X = x) > 0.5$$\n",
    "    $$Pr(default = Yes | X = x) > 0.2$$\n",
    "<img src=\"images/p145 table 4.z.png\" width = \"65%\"/>    \n",
    "    - 이를통해, 실제 default 대상에 대한 predict %는 24.3% → 58.6%로 향상됨 (물론, 반대급부의 예측손실 발생함)\n",
    "* 이러한 tradeoff는 다음과 같음\n",
    "<img src=\"images/p147 figure 4.7.png\" width = \"65%\"/>\n",
    "    - Bayes' classifier는 threshold = 0.5를 사용하며, 이것은 overall error rate이 가장 낮다고 알려짐\n",
    "\n",
    "\n",
    "* ROC Curve는 threshold 범위에 따라 2가지 error를 확인가능한 도구 (ROC : receiver operating characteristics)\n",
    "    - True Positive와 False Positive를 확인가능함\n",
    "    \n",
    "    1. TP : True Positive : 실제로 Positive인데, Positive로 예측\n",
    "    2. FP : False Positive : 실제로 Negative인데, Positive로 예측\n",
    "<img src=\"images/p148 figure 4.82.png\" width = \"65%\"/>\n",
    "    \n",
    "    - 즉, ROC Curve를 보면, threshold의 조정을 통해 TP, FP를 조정/변경할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.4.4 Quadratic Discriminant Analysis\n",
    "* LDA는 observation이 multivariate Gaussian distribution에서 추출된 class specific mean vector와 K개 모든 class에 공통인 covariance matrix를 가진다고 가정\n",
    "* QDA (Quadratic discriminant analysis)는 여기에서 각 class별로 각각의 covariance matrix를 가진다고 가정\n",
    "* 즉, $X \\sim N(\\mu_k, \\Sigma_k)$\n",
    "* Bayes' classfier는\n",
    "  $$\\delta_k(x) = -\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k) - \\frac{1}{2}log|\\Sigma_k| + log\\pi_k$$\n",
    "  $$=-\\frac{1}{2}x^T\\Sigma_k^{-1}x + x^T\\Sigma_k^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma_k^{-1}\\mu_k - \\frac{1}{2}log|\\Sigma_k| + log\\pi_k$$\n",
    "  가 되어, QDA는 $x$에 대한 quadratic function임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.5 A Comparison of Classification Methods\n",
    "* Logistic regression, LDA, QDA와 KNN classification\n",
    "* LDA framework : $$log\\left(\\frac{p_1(x)}{1-p_1(x)}\\right) = log\\left(\\frac{p_1(x)}{p_2(x)}\\right) = c_0 + c_1x$$\n",
    "* logistic regression : $$log\\left(\\frac{p_1}{1-p_1}\\right) = \\beta_0 + \\beta_1x$$\n",
    "에서, 모두 $x$에 linear하며, 둘 간의 차이는 단지 다음과 같음\n",
    "\n",
    "    1. $\\beta_0, \\beta_1$은 maximum likehood를 활용하여 estimation\n",
    "    2. $c_0, c_1$은 normal distribution의 mean과 variance를 활용하여 estimation\n",
    "    \n",
    "* LDA, logistic regression은 fitting procedure가 다른 것일뿐\n",
    "* 참고 : KNN의 경우 LDA, logistic regression과는 다른 non-parametric approach임 (plurality : 최대득표수 기반)\n",
    "    - 따라서, class간 decision boundary가 non-linear인 경우 매우 뛰어날 것\n",
    "    - 단, predictor importance를 알 수 없음, predictor의 coefficient를 알 수 없음\n",
    "\n",
    "* 따라서, QDA는 KNN과 LDA/ logistic regression간의 상호보완책으로 볼 수 있음 (어느정도의 decision boundary의 존재를 가정함)\n",
    "* 아래의 예와 같이 어떠한 방법도 모든 상황에서 우수한 performance를 제공하지는 않음\n",
    "    \n",
    "    1. 만약, decision boundary가 linear라면 LDA, logistic regression이 우수한 결과를 제공하며\n",
    "    2. 혹은, boundary가 non-linear라면 QDA가 더 나은 결과를 제공\n",
    "    3. 또한, boundary가 굉장히 복잡하다면 non-parametric KNN이 가장 우수한 결과를 제공\n",
    "<img src=\"images/p152 figure 4.10.png\" width = \"65%\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
