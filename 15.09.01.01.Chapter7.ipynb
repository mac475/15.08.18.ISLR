{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\", \"malgun gothic\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width: 100%;\n",
       "    }\n",
       "    ul {\n",
       "        line-height: 145%;\n",
       "        font-size: 90%;\n",
       "    }\n",
       "    li {\n",
       "        margin-bottom: 0.2em;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top: 12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width: 90%;\n",
       "//        margin-left:auto;\n",
       "//        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mac475의 ipython 표준 style을 적용함\n",
    "from IPython.core.display import HTML\n",
    "styles = open(\"styles/custom.css\", \"r\").read()\n",
    "HTML( styles )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#7. Moving Beyond Linearity\n",
    "\n",
    "* Chap.6의 linear model 향상은 model의 complexity (flexibility) reduce를 통해 variance를 향상시킴으로써 수행됨\n",
    "* Chap.7의 Linearity 확장은 다음과 같이 수행\n",
    "    * Polynominal regression : original predictor 대상의 power(제곱) 통해 extra predictor를 추가\n",
    "    * Step function : variable range를 K개 distinct region으로 구분, qualitative variable을 생성/활용 (구간 function fitting)\n",
    "    * Regression splines : 상기 polynominal, step function의 확장\n",
    "    * Smoothing splines : regression splines와 유사\n",
    "    * Local regression : splines와 유사\n",
    "    * Generalized additive models : multiple predictors 위한 확장된 방법 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##7.1 Polynominal Regression\n",
    "* linear regression의 extend : polynominal regression\n",
    "    $$y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i \\quad → \\quad y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2 + \\beta_3x_i^3 + \\cdots + \\beta_dx_i^d + \\epsilon_i$$\n",
    "* polynominal 역시 predictor $x_i, xi^2, x_i^3, \\cdots, x_i^d$ 대한 standard linear model이므로 least squares estimation 가능\n",
    "* 일반적으로, $d \\le 3, 4$ 정도를 활용 $\\because$ 지나치게 d값이 큰 경우 flexibility 급상승\n",
    "* 아래의 경우, wage에 대한 age의 degree-4 polynominal function\n",
    "    <img src=\"images/p267 figure 7.01.png\" width = \"65%\"/>\n",
    "    $$Logistic\\,function\\,:\\, P(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}이므로,$$\n",
    "    $$Pr(y_i > 250|x_i) = \\frac{exp(\\beta_0+\\beta_1x_i+\\beta_2x_i^2+\\cdots+\\beta_dx_i^d)}{1+exp(\\beta_0+\\beta_1x_i+\\beta_2x_i^2+\\cdots+\\beta_dx_i^d)}$$\n",
    "    과 같이 표현가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##7.2 Step Function\n",
    "* Polynominal function의 사용은 기본적으로 X에 대한 non-linear function을 의미\n",
    "* 대안으로써, step function 활용가능\n",
    "    - X의 범위를 bin 단위로 break하여, ordered categorical variable로 만들어 활용\n",
    "    <img src=\"images/p268 functions.png\" width = \"45%\"/>\n",
    "    - 이때, $I(\\cdot)$은 indicator function (조건충족시 1, 아니면 0)\n",
    "* Step function의 linear model은 다음과 같음\n",
    "    $$y_i = \\beta_0 + \\beta_1C_1(x_i) + \\beta_2C_2(x_i) + \\cdots + \\beta_KC_K(x_i) + \\epsilon_i$$\n",
    "    - 이 때, $C_1, C_2, \\cdots, C_K$ 중 최소한 1개는 non-zero임\n",
    "    - 또한, logistic function은 다음과 같이 표현가능\n",
    "    $$Pr(y_i > 250|x_i) = \\frac{exp(\\beta_0 + \\beta_1C_1(x_i) + \\beta_2C_2(x_i) + \\cdots + \\beta_KC_K(x_i))}{1+exp(\\beta_0 + \\beta_1C_1(x_i) + \\beta_2C_2(x_i) + \\cdots + \\beta_KC_K(x_i))}$$\n",
    "    <img src=\"images/p269 figure 7.2.png\" width = \"65%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##7.3 Basis Functions\n",
    "* Basis function : X에 적용가능한 가장 일반적 function : polynominal, step function 모두 basis function의 상세화임\n",
    "    $$y_i = \\beta_0 + \\beta_1b_1(x_i) + \\beta_2b_2(x_i)+\\beta_3b_3(x_i)+\\cdots+\\beta_Kb_K(x_i)+\\epsilon_i$$\n",
    "    - polynominal의 경우, $b_j(x_i) = x_i^j$\n",
    "    - step function (piecewise constant function)의 경우 : $b_j(x_i) = I(c_j \\le x_i < c_{j+1})$\n",
    "    - 즉, 요약하면 predictor $b_1(x_i),\\,b_2(x_i), \\cdots, b_K(x_i)$에 대한 standard linear model이며, least square통해 fitting 가능\n",
    "    - 따라서, Chapter 3에서 언급된 standard error, F-statistic 등 활용가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##7.4 Regression Splines\n",
    "###7.4.1 Piecewise Polynominals\n",
    "* 먼저, piecewise란, 링크[https://en.wikipedia.org/wiki/Piecewise] 참고\n",
    "    - *In mathematics, a piecewise-defined function (also called a piecewise function or a hybrid function) is a function which is defined by multiple sub functions, each sub function applying to a certain interval of the main function's domain (a sub-domain). Piecewise is actually a way of expressing the function, rather than a characteristic of the function itself, but with additional qualification, it can describe the nature of the function. For example, a piecewise polynomial function is a function that is a polynomial on each of its sub-domains, but possibly a different one on each.*\n",
    "* (range of X 대상 High-degree polynominal fitting 대신) piecewise polynominal regression은 다음과 같음\n",
    "    - $X$의 여러 region에 대해\n",
    "    - separate된 low-degree polynominal fitting\n",
    "* 즉, piecewise cubic(3차) polynominal regression model이 다음과 같은 경우,\n",
    "    $$y_i = \\beta_0+\\beta_1x_i+\\beta_2x_i^2+\\beta_3x_i^3+\\epsilon_i$$\n",
    "    - 각 coefficients인 $\\beta_0, \\beta_1, \\beta_2, \\beta_3$가 $X$의 주어진 범위에 따라 다른 경우, 이러한 point를 $knots$라 함\n",
    "    - 수식은 다음과 같이 표현가능\n",
    "    $$\n",
    "    y_i = \\left\\{\n",
    "                    \\begin{array}{rl}\n",
    "                           \\beta_{01}+\\beta_{11}x_i+\\beta_{21}x_i^2+\\beta_{31}x_i^3+\\epsilon_i &\\mbox{ if $x_i < c$}\n",
    "                           \\\\\n",
    "                           \\beta_{02}+\\beta_{12}x_i+\\beta_{22}x_i^2+\\beta_{32}x_i^3+\\epsilon_i &\\mbox{ if $x_i \\ge c$}\n",
    "              \\end{array}\n",
    "      \\right.\n",
    "    $$\n",
    "\n",
    "    - 요약하면, 두개의 polynominal function에 대한 fitting임 (using least squares)\n",
    "    - knot를 많이 사용하게 되면, more flexible piecewise polynominal이 됨\n",
    "\n",
    "###7.4.2 Constraints and Splines\n",
    "* cubic spline\n",
    "    - 일반적으로, K nots를 가지는 cubic spline은 4+K degrees of fredom을 가짐\n",
    "    <img src=\"images/p272 figure 7.3.png\" width = \"65%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###7.4.3 The Spline Basis Representation\n",
    "* regression spline : K개의 knot를 갖는 cubic spline은 다음과 같이 표현가능\n",
    "    $$y_i = \\beta_0+\\beta_1b_1(x_i)+\\beta_2b_2(x_i)+\\cdots+\\beta_{K+3}b_{K+3}(x_i)+\\epsilon_i$$\n",
    "* 이를 활용, cubic spline 표현시, 3차다항 ($x, x^2, x^3$)과 knot에 대한 truncated power basis function 추가\n",
    "    $$\n",
    "    h(x,\\xi) = (x-\\xi)_+^3 = \\left\\{\n",
    "                                     \\begin{array}{rl}\n",
    "                                         (x-\\xi)^3 &\\mbox{ if $x > \\xi$}\n",
    "                                     \\\\\n",
    "                                     0 &\\mbox{ otherwise }\n",
    "                                     \\end{array}\n",
    "                             \\right.\n",
    "    $$\n",
    "    - $\\xi$는 knot이며, 정의된 truncated power basis function을 regression spline 적용시 다음과 같음 (K = 1, cubic으로 가정)\n",
    "        $$y_i = \\beta_0+\\beta_1b_1(x_i)+\\beta_2b_2(x_i)+\\beta_3b_3(x_i)+\\beta_4h(x,\\xi)+\\epsilon_i \\quad (b_j(x_i)=x_i^j)$$\n",
    "        $$\\therefore y_i = \\beta_0+\\beta_1x_i^1+\\beta_2x_i^2+\\beta_3x^3+\\beta_4h(x,\\xi)+\\epsilon_i$$\n",
    "    - 과 같이 정리되어 $\\xi$에서 discontinuity 발생\n",
    "* K개 knot 대상, cubic spline fitting시, intercept와 3+K predictor 대상 least square 수행하게 됨 : K+4개 coefficient\n",
    "    - K+4 degrees of freedom\n",
    "* natural spline :  boundary에서 linear해야하는 boundary constraints를 갖는 regression spline (아래참고)\n",
    "    - 일반적으로 natural spline이 stable한 estimation 수행\n",
    "    <img src=\"images/p274 figure 7.4.png\" width = \"75%\"/>\n",
    "    - 생각해보면, data의 변동성이 큰 곳에서 knot가 존재하고, 그 외에서는 linear estimation 하는 것이 효과적일 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###7.4.4 Choosing the Number and Locations of the Knots\n",
    "* 적절한 knot의 위치의 고려가 필요함\n",
    "* 일반적으로 regression spline은 knot가 많이 포함된 영역에서 flexible할 것임\n",
    "* 따라서, function의 변동성이 큰 곳에 knot를 많이 배치하고, 그렇지 않은 곳에 knot를 적게 배치하는 것이 바람직\n",
    "* 하지만, 실무적으로는 knot를 균등하게 배치하곤 함 (방법 : degrees of freedom을 지정)\n",
    "* spline에 있어 최적 knot 개수, 혹은 최적 degrees of freedom 찾기위해 cross-validation 활용 RSS 기준으로 찾음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###7.4.5 Comparison to Polynominal Regression\n",
    "* Regression spline이 polynominal regression을 superior함\n",
    "    - polynominal : flexibility를 위해서는 high degree가 되어야 함 (즉, 지수가 증가해야 함. $x^2, x^3, \\cdots$)\n",
    "    - spline : flexibility를 위해서는 (degree는 고정시키고) knots 개수를 증가시킴\n",
    "* Splines also allow us to place more knots, and hence flexibility, over regions where the function f seems to be changing rapidly, and fewer knots where f appears more stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##7.5 Smoothing Splines\n",
    "###7.5.1 An Overview of Smoothing Splines\n",
    "* dataset에 대해 smooth curve를 fitting시, RSS를 최소화하는 것이 목표임\n",
    "    $$RSS = \\sum\\nolimits_{i=1}^n(y_i-g(x_i))^2$$\n",
    "* 이때, what we really want is function g that makes RSS small, but is also smooth임\n",
    "    - smooth : 미분가능하고 연속인 함수를 의미\n",
    "* g가 smooth임을 확보하기 위해, 아래를 minimize하는 function g를 찾는 것이 방법임\n",
    "    $$\\underbrace{\\sum\\limits_{i=1}^n(y_i-g(x_i))^2}_{\\mbox{loss function}}+ \\underbrace{\\lambda\\int g^{\\prime\\prime}(t)^2dt}_{\\mbox{penalty term}}\\quad(\\lambda : tuning\\,\\,parameter,\\,\\lambda \\ge 0)$$\n",
    "    - 이를 \"Loss + Penalty\" formulation이라함 (Chap.6의 ridge, lasso 참고)\n",
    "    - $g^{\\prime}(t)$ : slope (기울기), $g^{\\prime\\prime}(t)$ : slope (기울기)의 변량\n",
    "    - 따라서, $g^{\\prime\\prime}(t)$는 roughness의 척도로 활용가능\n",
    "    - $\\int g^{\\prime\\prime}(t)^2$는 function $g^{\\prime}(t)$의 총 변량합으로 볼 수 있음\n",
    "        - 만약, g가 smooth하다면, $g^{\\prime}(t)$은 상수(constant)이고, $\\int g^{\\prime\\prime}(t)^2$는 매우 작음\n",
    "        - g가 jumpy하다면, $g^{\\prime}(t)$는 크게 변화를 보이며, $\\int g^{\\prime\\prime}(t)^2$는 매우 커짐\n",
    "    - $\\lambda \\int g^{\\prime\\prime}(t)^2dt$에 있어서, 상기의 값(RSS)을 최소화해야 하므로,\n",
    "        - $\\lambda$가 커지면, $\\int g^{\\prime\\prime}(t)^2$가 작아져야 하므로, g는 smoother해짐\n",
    "        - $\\lambda = 0$이면, penalty term의 no effect, g는 jumpy해지고, RSS를 최소화해야 하므로, training data에 정확히 일치하게 됨\n",
    "        - $\\lambda → \\infty$이면, g는 완전히 smooth되어야 함 (straight line으로...) → 이때, $g(t)$는 linear least square line이 됨\n",
    "    - 즉, $\\lambda$는 bias-variance trade-off를 통해 smoothing spline을 조절\n",
    "* 이때, RSS를 최소화하는 $g(x)$는 knots $(x_1, \\cdots, x_n)$를 가지는 natural cubic spline임\n",
    "    - 정확히는, natural cubic spline의 shrunken version이며 $\\lambda$를 통해 shrinkage를 조절함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###7.5.2 Choosing the Smoothing Parameter $\\lambda$\n",
    "* 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##7.6 Local Regression\n",
    "* Local regression : non-linear function을 fitting하기위한 또다른 방법\n",
    "    - target point $x_0$에서 nearby training observation을 이용하여 fitting하는 방법\n",
    "    - 즉, least square에 KNN적 방법이 도입됨\n",
    "    <img src=\"images/p281 figure 7.9.png\" width = \"75%\"/>\n",
    "    - Local regression에서 span s가 중요 : non-linear fit에 대한 flexibility를 조절\n",
    "        - s가 작으면, more local, wiggly\n",
    "        - s가 크면, training data에 대해 globally fitting됨\n",
    "* Local Regression의 수행 Algorithm\n",
    "    - $x_0$에 대해 fraction $s = k / n$을 가까운 순서대로 gather\n",
    "    - $K_{i0} = K{x_i, x_0}$를 각 neighbor들에 배정 : 이때, $x_0$에서 가장 먼 곳은 0, 가장 가까운 곳은 highest weight\n",
    "        - 이때, k개를 제외한 neighbors는 0 weight를 가짐\n",
    "    - 다음과 같이 aforementioned weights를 활용하여, weighted least squares regression fitting, 다음을 최소화\n",
    "        $$\\sum\\limits_{i=1}^nK_{i0}(y_i-\\beta_0-\\beta_1x_i)^2$$\n",
    "    - $x_0$에 대해 fitted value는 $\\hat{f}(x_0) = \\hat{\\beta}_0+\\hat{\\beta}_1x_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##7.2 Generalized Additive Models\n",
    "* GAMs는 각 variables에 대한 non-linear function을 허용함으로써 standard linear model을 확장하는 framework임\n",
    "    - quantitative, qualitative response 모두에 적용가능\n",
    "\n",
    "###7.7.1 GAMs for Regression Problems\n",
    "* multiple linear regression은 다음과 같음\n",
    "    $$y_i = \\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\cdots+\\beta_px_{ip}+\\epsilon_i$$\n",
    "* 이때, feature-response간 non-linear 허용위해, 각 component $\\beta_jx_{ij}$를 non-linear function $f_j(x_{ij})$로 replace,\n",
    "    $$y_i = \\beta_0+\\sum\\limits_{j=1}^pf_j(x_{ij})+\\epsilon_i = \\beta_0+f_1(x_{i1})+f_2(x_{i2})+\\cdots+f_p(x_{ip})+\\epsilon_i$$\n",
    "    - 이를 additive model이라고도 함. $\\because$ 각 $X_j$에 대해 각각의 $f_j$를 계산하여 add together함\n",
    "* 가령, wage의 경우 다음과 같이 표현가능\n",
    "    $$wage = \\beta_0 + f_1(year) + f_2(age)+f_3(education) + \\epsilon$$\n",
    "\n",
    "\n",
    "####Pros and Cons of GAMs\n",
    "* GAM은 각 $X_j$에 대해 non-linear $f_j$ fitting을 허용\n",
    "* non-linear fit은 response Y에 대한 더 정확한 prediction 제공\n",
    "* model이 additive하므로, inference가 용이\n",
    "* function $f_j$의 smoothness는 degrees of freedom의 summarization\n",
    "* limitation : model이 기본적으로 additive라는 점. 즉, variables가 서로 interaction이 있는 경우 누락됨\n",
    "    - 따라서, manually하게 $X_j \\times X_k$ additional predictor를 추가하거나, $f_{jk}(X_j, X_k)$와 같은 function을 추가하여 해소\n",
    "\n",
    "###7.7.2 GAMs for Classification Problems\n",
    "* GAM은 response Y가 qualitative이 경우에도 적용가능\n",
    "* 다음과 같은 logistic regression에서\n",
    "    $$log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0+\\beta_1X_1+\\beta_2X_2+\\cdots+\\beta_pX_p$$\n",
    "    - 이고, 이를 non-linear relationship으로 허용시 다음과 같이 변환가능\n",
    "    $$log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0+f_1(X_1)+f_2(X_2)+\\cdots+f_p(X_p)$$\n",
    "* 각 개인수입이 연간 250,000달러 이상인 probability를 predict시 다음과 같은 형태를 fitting\n",
    "    $$log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0+\\beta_1 \\times year + f_2(age) + f_3(education)$$\n",
    "    where,\n",
    "    $$p(X) = Pr(wage > 250|year, age, education)$$\n",
    "* 다음의 logistic regression 결과를 보면, year에 대해 linear, age에 대해 smoothing spline 확인가능\n",
    "    <img src=\"images/p288 figure 7.14.png\" width = \"75%\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
