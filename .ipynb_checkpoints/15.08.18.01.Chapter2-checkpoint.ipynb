{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\", \"malgun gothic\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width: 100%;\n",
       "    }\n",
       "    ul {\n",
       "        line-height: 145%;\n",
       "        font-size: 90%;\n",
       "    }\n",
       "    li {\n",
       "        margin-bottom: 0.2em;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top: 12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width: 90%;\n",
       "//        margin-left:auto;\n",
       "//        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mac475의 ipython 표준 style을 적용함\n",
    "from IPython.core.display import HTML\n",
    "styles = open(\"styles/custom.css\", \"r\").read()\n",
    "HTML( styles )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Statistical Learning\n",
    "##2.1 What Is Statistical Learning\n",
    "* 일반적으로 *p*개의 predictors $X_1, X_2, \\cdots X_p$에 대해 quantitative response Y를 predicting시 다음과 같이 표현가능\n",
    "    $$Y = f(X) + \\epsilon$$\n",
    "    - 이때, $f(X)$는 $X_1, \\cdots, X_p$에 대한 function이며, $\\epsilon$은 random error term임\n",
    "* statistical learning은 $f$를 estimating하기위한 접근이라고 표현할 수 있음\n",
    "\n",
    "###2.1.1 Why Estimate f?\n",
    "* function $f$를 estimation하는 이유\n",
    "    - prediction\n",
    "    - inference\n",
    "\n",
    "####Prediction\n",
    "* Prediction의 접근은 $\\hat{Y} = \\hat{f}(X)$를 통해, $\\hat{Y}$를 predict함으로써 수행\n",
    "* $Y$에 대한 prediction으로써의 $\\hat{Y}$의 정확도, accuracy는 2가지 quantity에 의존\n",
    "    - reducible error, irreducible error\n",
    "    - reducible error는 function $hat{f}$의 accuracy 향상을 통해 줄일 수 있음\n",
    "    - 하지만, irreducible error는 줄일 수 없음\n",
    "        - 어째서, $\\epsilon > 0$인가? : $\\epsilon$에는 $Y$의 predicting시 유용한, unmeasurable variables가 포함되어 있으므로...\n",
    "* 이는 다음과 같이 표현가능,\n",
    "    $$E(Y-\\hat{Y})^2 = E|f(X) + \\epsilon - \\hat{f}(X)|^2\n",
    "      \\\\\n",
    "      \\quad \\quad \\quad \\quad \\quad \\quad= |f(X)-\\hat{f}(X)|^2 + Var(\\epsilon)\n",
    "    $$\n",
    "    - 이때, $|f(X)-\\hat{f}(X)|^2$ 부분이 향상이 가능한 *reducible error*이고, $Var(\\epsilon)$ 부분이 향상이 불가능한 *irreducible error*임\n",
    "    - 대부분의 statistical learning에서의 주안점은, reducible error를 줄이는 것에 있음\n",
    "\n",
    "####Inference\n",
    "* Data의 분석시, response $Y$가 $X_1, \\cdots, X_p$의 변화에 의해 어떻게 영향받게 되는지 이해가 필요한 경우가 있으며, 이를 Inference, 추론이라 함\n",
    "\n",
    "\n",
    "* Data분석의 궁극적 목표가 prediction인지, inference인지, 혹은 둘의 combination인지에 따라 $f$ estimation시 서로 다른 방법으로 접근가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2.1.2 How Do We Estimate f?\n",
    "* statistical learning은 *parametric*, *non-parametric* 접근방법이 있음\n",
    "\n",
    "\n",
    "####Parametric Methods\n",
    "* Parametric 접근방법은 2단계로 진행\n",
    "    1. $f$를 다음과 같이 가정 : $f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p$\n",
    "    2. training data를 활용하여 fitting (trainig)함으로써, $\\beta_0, \\beta_1, \\cdots, \\beta_p$를 estimation\n",
    "        - 이때, fitting시 가장 일반적인 접근은 *(ordinary) least square*를 활용\n",
    "\n",
    "\n",
    "    * 이는, true $f$에 대한 estimation을 parameter set에 대한 estimation으로 간소화해줌\n",
    "    * 하지만, 이렇게 estimation된 model은 true unknown $f$와 일치하지는 않는 disadvantage 존재\n",
    "    * 많은 parameter의 활용과 polynominal의 적용은 상기의 linear model에 비해 flexible 접근방법을 제공하나 training data에 대한 *overfitting* 가능\n",
    "        - 즉, model이 training data내의 각종 error 및 noise를 followg함으로써 발생하는 현상으로 요약\n",
    "\n",
    "\n",
    "####Non-parametric Methods\n",
    "* Non-parametric method는 function $f$에 대한 명시적 assumption을 하지 않음\n",
    "* 대신, data point에 최대한 가까운 $f$를 찾고자 노력함\n",
    "    - 이러한, thin-plate형태의 fitting을 위해 smoothness를 적용\n",
    "         \n",
    "         \n",
    "         \n",
    "<img src=\"images/p22 figure 2.4.png\" width = \"45%\"/>\n",
    "<img src=\"images/p23 figure 2.5.png\" width = \"45%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability\n",
    "* Linear regression : linear function만을 생성하므로, 상대적으로 flexibility가 낮음\n",
    "* Thin plate spline : flexibility 높음\n",
    "* 의문 : 어째서, flexibility가 높은 방법대신, 각종 제약이 있는 linear 계열의 model을 수행하는가? \n",
    "    - 분석의 목적이 inference인 경우, linear와 같이 restrictive한 model이 쉽고, 용이함 $Y$와 $X_1, \\cdots, X_p$간의 관계해석이 용이\n",
    "    - 반면, spline과 같이 flexibility가 높은 경우 estimated $f$의 complexity 높아져 각 predictor들이 response와 어떠한 관계인지 이해하기 어려움\n",
    "    - 아래는 flexibility와 interpretability간의 trade-off\n",
    "    <img src=\"images/p25 figure 2.7.png\" width = \"55%\"/>\n",
    "\n",
    "\n",
    "    - *lasso* : 일부 coefficient를 zero화함으로써, 결과적으로 linear regression보다 flexibility가 낮고, interpretability가 높음\n",
    "    - *GAM : Generalized Additive Models* : linear model을 extend하여 flexible\n",
    "    - *bagging, boosting, support vector machines (with non-linear kernels)* : highly flexible, interpret 어려움\n",
    "\n",
    "* Data 분석의 목적에 따라, 즉, inference, prediction에 따라 flexibility, interprebility 수준을 선택/ 활용해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2.1.4 Supervised Versus Unsupervised Learning\n",
    "* statistical learning은 대부분 다음과 같이 구분\n",
    "    - *supervised learning* : $x_i, \\, i = 1, \\cdots, n$에 있어서, 이것과 associated된 $y_i$가 존재\n",
    "        - predictor들을 response와 관련하여 model을 fitting\n",
    "        - future observation들에 대한 predicting 목적 : prediction\n",
    "        - predictor와 response간 relationship을 understanding : inference\n",
    "        - linear regression, logistic regression, GAM, boosting, support vector machines\n",
    "    - *unsupervised learning* : $x_i, \\, i = 1, \\cdots, n$에 있어서, 이것과 associated된 $y_i$가 없음\n",
    "        - 주어진 dataset을 subgroup화 : *clustering analysis*\n",
    "    - *semi-supervised learning* : *n*개 observation ($n > m$)에서, *m*개는 response가 존재하나, *n-m*개는 response가 존재치 않는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2.1.5 Regression Versus Classification Problems\n",
    "* Variables는 *quantitative*와 *qualitative (categorical)*로 구분가능\n",
    "    - quantitative : numerical value : 일반적으로 regression problem\n",
    "    - qualitative : classes, categories : 일반적으로 classification problem\n",
    "    - 하지만, qualitative value에 대한 class probability를 estimation할 수 있다는 관점에서 regression으로도 수행가능\n",
    "    - 더불어, qualitative predictor 역시 analysis전에 적절하게 numerical value로 encoding하여 처리가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.2 Assessing Model Accuracy\n",
    "* statistical learning에 있어 모든 문제를 잘 해결하는 dominant한 하나의 방법은 없음\n",
    "\n",
    "###2.2.1 Measuring the Quality of Fit\n",
    "* 주어진 observation에 대해 response prediction이 실제 response와 얼마나 close한지 수량화필요\n",
    "    - MSE (mean squared error)\n",
    "$$MSE = \\frac{1}{n}\\sum\\limits_{i=1}^n(y_i-\\hat{f}(x_i))^2 = Ave (y_i-\\hat{f}(x_i))^2$$\n",
    "    - 단, lowest training MSE가 lowest test MSE를 보장하지는 못함\n",
    "    <img src=\"images/p31 figure 2.9.png\" width = \"65%\"/>\n",
    "    - 위와같이, training MSE가 model의 flexibility에 따라 낮아지더라도, test MSE는 U-Shape를 보임\n",
    "    - 이는, 모든 statistical learning 방법에서 공통적으로 보여지는 fundamental한 property임\n",
    "* Overfittingtraining MSE은 충분히 작으나, test MSE가 크다면 이를 *overfitting*이라 할 수 있음\n",
    "    - 이는 training data내의 pattern을 지나칠정도로 찾아냄으로써 training내의 각종 noise 및 우연적 결과까지도 반영해버린 결과\n",
    "* 즉, 낮은 test MSE를 확보하기 위한 노력이 필요하므로, cross-validation 활용통해, training data로 test data를 estimate함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2.2.2 The Bias-Variance Trade-Off\n",
    "* model flexibility에 따른 test MSE의 U-Shape 경향은 기본적으로 bias-variance trade off에 기인\n",
    "* test MSE는 다음과 같이 세가지 요소로 분해가능\n",
    "    - $\\hat{f}(x)$의 variance, $\\hat{f}(x)$의 bias의 제곱, error term $\\epsilon$의 variance\n",
    "    $$E\\left( y_0 - \\hat{f}(x_0)\\right)^2 = Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon)$$\n",
    "    - 훌륭한 model을 low variance와 low bias를 동시에 달성하는 것\n",
    "* Variance : trainind data set중에서 일부의 선택을 변경했을때, $\\hat{f}$가 변경되는 정도의 양\n",
    "    - 만약, 매우 flexible case 가정한다면, 하나, 혹은 두개의 observation의 training set 포함여부만으로도 결과 model : $\\hat{f}$의 변화량이 상당할 것\n",
    "    - 일반적으로, flexible한 model일수록 variance가 커짐\n",
    "* Bias : 실제 결과와 예측 결과의 차이\n",
    "* 일반적으로, flexible한 model/ 방법일수록 variance는 증가하며, bias는 감소\n",
    "    - 또한, 점차적으로 model의 flexibility를 증가시키는 경우, (자연히) bias는 variance의 증가에 비해 급격하게 감소 → 따라서, MSE 감소\n",
    "    - 하지만, 일정수준이 지나면, bias의 감소정도는 작아지고, 대신 variane의 증가가 발생 → 따라서, MSE 증가\n",
    "    - 이를 bais-variance trade-off라 함\n",
    "    <img src=\"images/p36 figure 2.12.png\" width = \"65%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2.2.3 The Classification Setting\n",
    "* Classification problem의 경우, response값이 *qualitative*이므로, MSE를 사용불가하여, error rate을 활용\n",
    "    $$\\frac{1}{n}\\sum\\limits_{i=1}^n I( y_i \\neq \\hat{y_i})$$\n",
    "    - 이때, $I( y_i \\neq \\hat{y_i})$를 indicator variable이라함\n",
    "\n",
    "####The Bayes Classifier\n",
    "* 위의 error rate을 낮추기 위해선, 정확도가 높은 다음과 같은 classifier의 확률값을 최대로 하도록 함 (이를, 조건부 확률이라함)\n",
    "    $$Pr(Y = j | X = x_0)$$\n",
    "    - predictor vector가 $x_0$인 경우, $Y = j$일 확률의 의미\n",
    "    - Bayes classifier라 함\n",
    "    <img src=\"images/p38 figure 2.13.png\" width = \"55%\"/>\n",
    "    - 위의 가운데 dashed line이 Bayes decision boundary임\n",
    "    - Bayes classifier를 사용시, Bayes error rate이 계산됨\n",
    "    - 이때, Bayes error rate은 다음과 같음 : $\\because$ Bayes classifier는 Pr이 최대로 되는 boundary이므로...\n",
    "        $$1-E\\left( max_jPr(Y=j | X = x_0) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####K-Nearest Neighbors\n",
    "* 이론적으론, Bayes classifier를 활용하여 qualitative predicting 가능하나, 실제론 불가능함\n",
    "* 따라서, KNN classifier 활용\n",
    "    - training data $x_0$에 가장 가까운 $K$개 point를 찾음 : 이를 $\\mathcal{N}_0$ 집합이라 함\n",
    "    - $\\mathcal{N}_0$내 각 point들에 대해 class $j$일 조건부 확률 판단\n",
    "    - 가장 큰 확률값의 class로 $x_0$를 assign\n",
    "    $$Pr(y=j | X = x_0) = \\frac{1}{K}\\sum\\limits_{i \\in \\mathcal{N}_0} I(y_i = j)$$\n",
    "* KNN에 있어, K값의 선택은 classifier에 큰 영향발휘\n",
    "    - K의 증가에 따라, flexibility 감소하며, linear한 형태의 boundary가 확보되므로, high-bias, low-variance인 boundary\n",
    "    - K의 감소에 따라, flexibitity 증가하며, flexible 형태의 low-bias, high-variance인 boundary\n",
    "* Regression과 마찬가지로, boundary의 flexibility 증가에 따라 U-Shape을 보임\n",
    "    <img src=\"images/p41 figure 2.16.png\" width = \"65%\"/>\n",
    "* Regression, classification 모두 적절한 flexibility를 확보하는 것이 매우 crucial함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
