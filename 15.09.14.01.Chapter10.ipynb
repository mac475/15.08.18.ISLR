{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\", \"malgun gothic\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width: 100%;\n",
       "    }\n",
       "    ul {\n",
       "        line-height: 145%;\n",
       "        font-size: 90%;\n",
       "    }\n",
       "    li {\n",
       "        margin-bottom: 0.2em;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top: 12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width: 90%;\n",
       "//        margin-left:auto;\n",
       "//        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mac475의 ipython 표준 style을 적용함\n",
    "from IPython.core.display import HTML\n",
    "styles = open(\"styles/custom.css\", \"r\").read()\n",
    "HTML( styles )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#10. Unsupervised Learning\n",
    "* Unsupervised Learning : *n*개 observation $X_1, X_2, \\cdots X_p$ 대해, response Y 예측아닌, data를 visualization하고, subgroup화 하는 작업수행\n",
    "    - PCA (Principal components analysis) : data visualization, 혹은 supervised learning 전의 pre-processing\n",
    "    - Clustering : subgroup화\n",
    "\n",
    "##10.1 The Challenge of Unsupervised Learning\n",
    "* Unsupervised learning은 종종 EDA(Exploratory data analysis)의 일종으로 보여지기도 함\n",
    "\n",
    "##10.2 Principal Components Analysis\n",
    "* Chap.6.3.1에서 principal components regression을 수행한 바 있음\n",
    "* 많은 variables 가진 data set에서, principal components는, data를 가장 잘 설명할 수 있는, 좀 더 작은 수의 variables로 축소하는 역할수행\n",
    "* PCA는\n",
    "    - principal components가 계산되는 process를 의미\n",
    "    - data를 이해하기 위해 이러한 component들이 사용되는 것\n",
    "    - data visualization의 tool로 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###10.2.1 What Are Principal Components\n",
    "* *p* feature 가지는 *n* observation을 visualization하는 경우, *p* feature중 2개씩 선택하여 scatterplot을 그릴 수 있음\n",
    "    - 이때의 경우의 수는, (반복하지만) 다음과 같음\n",
    "        $$\n",
    "        {_p}P_r = \\frac{n!}{(n-r)!}, \\quad {_n}C_r = \\frac{{_p}P_r}{r!} = \\frac{n!}{r!(n-r)!}\n",
    "        \\\\\n",
    "        \\dbinom{p}{2} = {_p}C_2 = \\frac{{_p}P_2}{2!} = \\frac{p!}{2!(p-2)!} = \\frac{p(p-1)(p-2)\\cdots 2}{2 (p-2)(p-1)\\cdots 2} = \\frac{p(p-1)}{2}$$\n",
    "    - p값의 증가시 필요한 scatterplot의 수는 비약적으로 증가하게 됨\n",
    "    - 따라서, data에 대한 특성을 가장 잘 capture할 low-dimensional한 것을 찾을 필요성 있음 → PCA가 제공\n",
    "    - PCA는 observation이 가장 크게 변하는 (vary하는) 양을 측정하여 interesting한 것을 찾아 그 dimension을 결과로 제공\n",
    "\n",
    "\n",
    "* $X_1, X_2, \\cdots, X_p$의 first principal component는,\n",
    "    - 아래,  normalized linear combination of the features중 가장 큰 variance 가지는 것\n",
    "    $$Z_1 = \\phi_{11}X_1 + \\phi_{21}X_2 + \\cdots + \\phi_{p1}X_p$$\n",
    "    - 이때, *normalized*에 의해, $\\sum_{j=1}^p\\phi_{j1}^2 = 1$이며,\n",
    "    - $\\phi_{11} \\cdots \\phi_{p1}$을 first principal component의 *loadings*라 함\n",
    "    - 이때, principal loading vector $\\phi_1 = (\\phi_{11} \\, \\phi_{21} \\, \\cdots \\, \\phi_{p1})^T$임\n",
    "* $n \\times p$의 data set $X$를 가정시, first principal component의 계산방법은\n",
    "    - 오로지, variance에만 관심이 있으므로,\n",
    "    - $X$내의 모든 variables는 mean zero라고 가정함 (즉, $X$의 column mean값도 zero임)\n",
    "    - 이때, $z_{i1} = \\phi_{11}x_{i1}+\\phi_{21}x_{i2}+\\cdots+\\phi_{p1}x_{ip}$의 variance를 가장 크게하는 linear combination을 찾음\n",
    "    - 단, $\\sum_{j=1}^p\\phi_{j1}^2 = 1$로 normalized\n",
    "* 이를 활용, 다음의 optimization problem solve함\n",
    "    $$maximize_{\\phi_{11}, \\cdots \\phi_{p1}} \\left\\{ \\frac{1}{n}\\sum\\limits_{i=1}^n \\left( \\sum\\limits_{j=1}^p \\phi_{j1}x_{ij}\\right)^2 \\right\\}\n",
    "    \\\\\n",
    "    subject \\, to \\quad \\sum\\limits_{j=1}^p \\phi_{j1}^2 = 1\n",
    "    $$\n",
    "    - 위의 식은 $\\frac{1}{n}\\sum_{i=1}^n z_{i1}^2$과 같이 표현가능\n",
    "    - 이때, (가정에 의해) $\\frac{1}{n}\\sum_{i=1}^n x_{ij} = 0$이므로, $z_{11} \\cdots z_{n1}$의 평균도 0 (이라고 함. 증명안했음)\n",
    "    - 위 식을 maximizing하는 것은 결국 $z_{i1}$의 $n$개 value : $z_{i1}, \\cdots , z_{n1}$이고, 이를 first principal component의 *scores*라 함\n",
    "    - data에 있어 가장 variance가 큰 feature space를 $\\phi_1 = (\\phi_{11} \\, \\phi_{21} \\, \\cdots \\, \\phi_{p1})^T$라 하면, *n*개의 observation $x_1, x_2, \\cdots, x_n$을 이 feature space의 direction으로 투사 (projection)시, projected된 값들이 principal component scores $z_{11}, \\cdots ,z_{n1}$임 (아래 참고) : green line이 principal component loading vector임\n",
    "    <img src=\"images/p230 figure 6.14.png\" width = \"65%\"/>\n",
    "\n",
    "* $X_1, X_2, \\cdots, X_p$의 second principal component는,\n",
    "    - $X_1, X_2, \\cdots, X_p$에 대해 $Z_1$과 *uncorrelated*인 maximal variance를 가지는 linear combination임\n",
    "    - second principal component scores는 $z_{i2}, \\cdots , z_{n2}$이며,\n",
    "    - $z_{i2} = \\phi_{12}x_{i1} + \\phi_{22}x_{i2} + \\cdots + \\phi_{p2}x_{ip}$이고\n",
    "    - 이때, second principal component loading vector $\\phi_2 = (\\phi_{12} \\, \\phi_{22} \\, \\cdots \\, \\phi_{p2})^T$임\n",
    "    - $Z_2$가 $Z_1$과 *uncorrelated*라는 것은 즉, $\\phi_2$가 $\\phi_1$과 수직이라는 의미임\n",
    "\n",
    "* 아래는 PCA통해 projected된 example이며, 귀납적으로 보아\n",
    "    - first principal component는 범죄와 관련성\n",
    "    - second principal component는 도시화와 관련성\n",
    "    - 있는 것으로 해석가능하므로, 이에 기반하여 각 도시의 범죄율/ 도시화 분석가능함\n",
    "    - 아래의 숫자는 principal component score와 loading vector ($0 \\le v \\le 1$)\n",
    "<img src=\"images/p378 figure 10.1.png\" width = \"65%\"/>\n",
    "    * 참고 : *biplot* : principal component의 score와 loading vector를 표현, 2개의 variable만으로 data를 scatterplot으로 표현함으로써 설명시도\n",
    "    * 참고 : [http://stats.stackexchange.com/questions/2038/interpretation-of-biplots-in-principal-components-analysis-in-r]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###10.2.2 Another Interpretation of Principal Components\n",
    "* principal component의 중요 두개념\n",
    "    - principal component loading vector : observation feature space에서 variance가 가장 큰 directions (분산, 벡터)\n",
    "    - principal component scores : *n* observations의 principal component loading vector로의 projections\n",
    "* principal component를 활용한 data의 interpretation\n",
    "    - *n* observation이 각각 가지는 *p*-dimension에서 가장 data를 잘 요약하여 설명할 수 있는 dimension을 찾아, *p*-dimension을 여기에 projection함으로써 해석력을 극대화함\n",
    "* *M* principal component score vector,*M* principal component loading vector 통해 *M*-dimensional 추정수행\n",
    "    $$x_{ij} \\approx \\sum\\limits_{m=1}^M z_{im}\\phi_{jm}$$\n",
    "    - 이때, $z_{im}$ : principal component score, $\\phi_{jm}$ : loading vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###10.2.3 More on PCA\n",
    "####Scaling the Variance\n",
    "* PCA에 대한 특성\n",
    "    1. PCA를 수행전, variables를 mean zero가 되도록 centered해야함\n",
    "    2. PCA 수행결과는 각 variables가 scaled되었는지에 dependency가 있음 (즉, variable에 특정 상수가 곱해졌는가)\n",
    "        - 일반적으로는 supervised/ unsupervised learning에 있어 variable에 대한 scaling은 결과에 영향 없음\n",
    "    3. 따라서, PCA 수행전 : 각 variables의 standard deviation = 1이 되도록 scaling해야함\n",
    "        - 이는, 각 variables가 일반적으로 서로 다른 단위(unit)으로 측정되는 것에 기인함 : 아래를 봐라\n",
    "        - 즉, 만약, 모든 variables의 단위가 같다면 이러한 scaling은 필요없음\n",
    "        <img src=\"images/p381 figure 10.3.png\" width = \"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Uniqueness of the Principal Components\n",
    "* principal component loading vector는 +/-부호에 관계없이 고유함\n",
    "* score vector는 +/- 부호에 관계없이 고유함\n",
    "\n",
    "####The Proportion of Variance Explained\n",
    "* PCA를 수행시, variables의 projection을 통해 손실되는 것 발생\n",
    "* 하지만, 주된 관심은 다음과 같을 것\n",
    "    - 전체 variable이 가지고 있는 variance에 대해서, 각 principal component를 통해 variance가 설명되는 수준/ 정도\n",
    "    - 이를, 각 principal component의 *proportion of variance explained (PVE)*라고 함\n",
    "    - 이때, total variance는,\n",
    "        $$\\sum\\limits_{j=1}^p Var(X_j) = \\sum\\limits_{j=1}^p \\frac{1}{n} \\sum\\limits_{i=1}^n x_{ij}^2$$\n",
    "    - *m*번째 principal component에 의한 variance explained는,\n",
    "        $$\\frac{1}{n}\\sum\\limits_{i=1}^n z_{im}^2 = \\frac{1}{n}\\sum\\limits_{i=1}^n \\left( \\sum\\limits_{j=1}^p \\phi_{jm}x_{ij}\\right)^2$$\n",
    "    - 따라서, *m*번째 principal component의 PVE는,\n",
    "        $$\\frac{\\sum_{i=1}^n \\left( \\sum_{j=1}^p \\phi_{jm}x_{ij} \\right)^2}{\\sum_{j=1}^p\\sum_{i=1}^n x_{ij}^2}$$\n",
    "    - *M*개 principal component까지의, 누적 *PVE*는 위 PVE를 *M*개 sum하여 계산\n",
    "    - 아래를 보면, first principal comp.가 약 62%, second principal comp.가 약 24% 정도의 variance 설명력을 제공하는 것을 확인가능\n",
    "    <img src=\"images/p383 figure 10.4.png\" width = \"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Deciding How Many Principal Components to Use\n",
    "* $n \\times p$인 data matrix $X$의 principal component의 개수 = $min( n-1, p)$\n",
    "* data에 대한 good understanding을 위해 필요한, 최소한의 principal component를 사용\n",
    "* 일반적으로, 적절한 principal component의 개수는 위의 scree plot을 통해 시각적으로 확인하곤 함\n",
    "    - 위 scree plot에서는 first, second principal component이후의 PVE가 급격히 감소하고 있음 : 이를 *elbow*라고 함\n",
    "* 하지만, scree plot기반의 시각적 탐색은 기본적으로 *ad hoc*적임\n",
    "    - 또한, principal component의 개수를 결정하기 위한 객관적 방법은 아직 정리되어 있지 않음\n",
    "    - 하나, 둘... 탐색적으로 적절한 principal component를 찾아가는 것이 가장 현실적이며, 이는 PCA의 EDA적 성격이 반영된 것이기도 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###10.2.4 Other Uses for Principal Components\n",
    "* regression, classification, clustering과 같은 다양한 통계적 방법에 있어,\n",
    "    - $M \\ll p$인, 주요 principal component를 대상으로\n",
    "    - $n \\times M$ matrix를 활용하는 것이\n",
    "    - $n \\times p$ matrix를 활용하는 것보다\n",
    "    - noise를 줄이고,\n",
    "    - (일반적으로 dataset의 signal은) 적은 수의 몇개의 principal component에 집중되곤 하므로 유용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##10.3 Clustering Methods\n",
    "* Clustering은 dataset으로부터 subgroup, 혹은 cluster를 찾는 것을 의미\n",
    "    - cluster 구분시, observation간의 유사성, 혹은 차이성에 대해 명확하게 사전 정의해야 함\n",
    "    - 대부분의 경우, domain-specific한 고려가 반영됨\n",
    "* PCA와 Clustering은 data를 simplify하는 유사성을 가지고 있으나,\n",
    "    - PCA : observation의 variance를 잘 설명가능하도록 적절한 low-dimensional representation을 찾음\n",
    "    - Clustering : observation간의 유사성을 찾음\n",
    "* 다음의 주요 두가지 clustering을 소개함\n",
    "    - K-means clustering : 사전에 정의한 cluster로 observation을 분류\n",
    "    - hierarchical clustering : 사전 정의한 cluster 없이, 계층적으로 구분하며, 최종적으로 *dendrogram (계통도)* 확보하게 됨 : 아래 참고\n",
    "    <img src=\"images/p386 dendrogram.png\" width = \"40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###10.3.1 *K*-Means Clustering\n",
    "* *K*-means는 data를 서로 overlapping이 없는 *K*개 cluster로 partitioning하는, 간단하고 elegant한 개념\n",
    "    1. 구분하고자 하는 cluster의 개수 *K*를 선정\n",
    "    2. *K*-means algoritm 활용하여 각 observation을 *K*개 중의 한개 cluster로 배정\n",
    "    <img src=\"images/p387 figure 10.5.png\" width = \"65%\"/>\n",
    "\n",
    "\n",
    "* *K*-means procedure는\n",
    "    0. $C_1, \\cdots, C_K$를 각각의 cluster라 하고,\n",
    "    1. $C_1 \\cup C_2 \\cup \\cdots \\cup C_K = { 1, \\cdots , n}$ : 즉, 각 observation은 *K*개의 cluster중 최소한 한개 cluster에는 속함\n",
    "    2. $k \\neq k^{\\prime}$에 대해서, $C_K \\cap C_{K\\prime} = 0$ : 즉, cluster간 overlapping은 없음\n",
    "\n",
    "\n",
    "* Good cluster : *within-cluster variance*가 작아야 함\n",
    "    - 이때, cluster $C_K$의 *within-cluster variance*인 $W(C_K)$는 cluster내 observation들이 서로 다른정도의 amount임. 따라서, 다음과 같이 표현\n",
    "    $$minimize_{C_1, \\cdots C_K}\\left\\{  \\sum\\limits_{k=1}^K W(C_K) \\right\\}$$\n",
    "    - 즉, 전체 *K*개 cluster들에 대한 *within-cluster variance*의 합을 가장 작게 하도록 각 observation들을 *K* cluster에 배정한다는 의미\n",
    "    - 이때, 위의 식을 풀기위해서는 우선 *within-cluster variance*를 다음과 같이 define :\n",
    "        $$W(C_K) = \\frac{1}{|C_K|} \\sum\\limits_{i, i^{\\prime} \\in C_K} \\sum\\limits_{j=1}^p (x_{ij} - x_{i^{\\prime}j})^2$$\n",
    "        - 참고 : $|C_K|$ : *k* cluster내 observation의 개수\n",
    "        - *k* cluster내 모든 observation들간의 squared Euclidean distance(유클리드거리제곱)합을 *k* cluster observation수로 나눈다는 의미\n",
    "    - 즉, 위 식을 combine하면,\n",
    "        $$minimize_{C_1, \\cdots C_K}\\left\\{  \\sum\\limits_{k=1}^K \\frac{1}{|C_K|} \\sum\\limits_{i, i^{\\prime} \\in C_K} \\sum\\limits_{j=1}^p (x_{ij} - x_{i^{\\prime}j})^2 \\right\\}$$\n",
    "    - 위의 식을 통해 minimize하기위한 algorithm은 다음과 같음\n",
    "    <img src=\"images/p388 algorithm 10.1.png\" width = \"65%\"/>\n",
    "    - 즉, 요약하면\n",
    "        1. 각 observation에 대해 임의의 cluster를 1 ~ *K*로 지정함\n",
    "        2. 다음을 반복함\n",
    "            (1) 각 *K* cluster에 대해, centroid(중심) 계산. 이때, *k* cluster centroid는 *k* cluster내 observation의 *p* feature vector의 mean값임\n",
    "            (2) 각 observation을 가장 가까운 centroid의 cluster로 update하여 배정. 이때, Euclidean diatance로 계산\n",
    "    - 이때, 결과가 더이상 변경되지 않는 시점이 *local optimum*이 완료된 시점\n",
    "    - 다음의 *K*-means algorithm progress를 참고\n",
    "    <img src=\"images/p389 figure 10.6.png\" width = \"55%\"/>\n",
    "    - *K*-means clustering의 명칭은 위 2-(a)에서 보여지듯, cluster centroid가 각 cluster내 observation의 mean값에 의해 계산되는 것에 기인\n",
    "    - *K*-means algorithm은 local optimum이며, 최종결과는 최초의 random cluster assignment에 의존성을 가지게 됨\n",
    "        - 따라서, 상이한 random configure를 적용하여 여러번 수행한 후, $W(C_K)$합을 minimize하는 solution을 찾아야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###10.3.2 Hierarchical Clustering\n",
    "* *K*-means clustering의 경우 cluster의 개수 *K*를 사전에 결정해야 한다는 문제점 있음\n",
    "* Hierarchical clustering의 경우 사전에 *K*값을 설정하지 않아도 된다는 것과 더불어 dendrogram이라는 tree형태 표현이 가능하다는 장점존재\n",
    "    - bottom-up으로 cluster가 형성됨\n",
    "\n",
    "\n",
    "####Interpreting a Dendrogram\n",
    "* dendrogram에서의 leaf는 각 observation을 의미, leaf는 fuse(녹다)되어 branch가 되고, branch끼리도 fuse됨\n",
    "* fusion이 먼저 될수록, observation의 유사성이 높으며, fusion이 늦게 될수록 차이가 크다고 볼 수 있음\n",
    "* 즉, dendrogram에 있어, fusion이 발생한 높이를 통해 observation간의 차이의 정도를 알 수 있음\n",
    "    - fusion이 bendrogram 아래에서 발생할수록 유사성이 높으며\n",
    "    - fusion이 위에서 발생할수록 차이가 큼\n",
    "* dendrogram에 있어서 2개 observation간의 유사성은, 2개 observation이 소속된 branch가 fusing된 vertical axis의 위치를 통해 파악가능\n",
    "    <img src=\"images/p393 figure 10.10.png\" width = \"55%\"/>\n",
    "* hierarchical clustering은 dendrogram에 horizontal cutting을 함으로써 구분가능\n",
    "* 이때, dendrogram에서의 cutting height는 *K*-means clustering에서의 *K*값을 통해 cluster 개수조절과 같은 의미\n",
    "    <img src=\"images/p392 figure 10.9.png\" width = \"65%\"/>\n",
    "* *hierarchical*이라는 단어는,\n",
    "    - bendrogram에서 낮은 cutting height 통해 확보된 cluster는 이보다 높은 cutting height 통해 확보된 cluster에 포함되는 계층성에 기인\n",
    "    - 단, 이러한 계층성이 항상 모든 dataset에서 잘 통한다고 볼 수 없다\n",
    "    - p394의 ex, 한개 집단을 국가단위, 성별단위, 연령대 단위 구분시 bendrogram기반의 hierarchical의 계층성이 좋은 결과를 주지못할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####The Hierarchical Clustering Algorithm\n",
    "* Hierarchical clustering dendrogram은\n",
    "    - 0. 우선, observation간의 dissimilarity(비유사도)를 측정 : 기본적으로 Euclidean distance를 활용\n",
    "    - 1. n개 observation이 각각 하나씩의 cluster인 상태로 시작\n",
    "    - 2. 가장 유사도가 높은 2개의 cluster를 fuse : 반복\n",
    "    - 3. 모든 cluster가 1개의 cluster로 통합될 때까지 수행후 종료\n",
    "\n",
    "\n",
    "* 다음은 Hierarchical clustering algoritm임\n",
    "    <img src=\"images/p395 figure 10.2.png\" width = \"55%\"/>\n",
    "\n",
    "\n",
    "* 각 cluster가 하나의 observation을 가진 경우의 dissimilarity 개념은 다소 쉬우나, 만약, 둘 중 하나의 cluster, 혹은 두 cluster 모두 여러 개의 observation을 가지고 있다면, dissimilarity를 어떻게 규정할 것인가에 대한 이슈가 발생\n",
    "    - *linkage* 활용 : 2개 그룹의 observation간의 dissimilarity를 정의 : complete, average, single, centroid (아래 참고)\n",
    "    <img src=\"images/p395 table 10.2.png\" width = \"55%\"/>\n",
    "    - Complete : 두 cluster간 거리중 멤버간 거리가 가장 긴 것을 활용\n",
    "    - Single : 두 cluster간 거리중 멤버간 거리가 가장 짧은 것을 활용\n",
    "    - Average : 두 cluster간 거리중 멤버간 거리의 평균을 활용\n",
    "    - Centroid : 두 cluster의 멤버간 multivariate mean간의 거리를 활용\n",
    "    - 참고 : 일반적으로 average와 complete이 선호됨 (single 대비)\n",
    "    - 최종 dendrogram은 적용된 *linkage* type에 따라 그 형태가 크게 달라짐 : 아래를 참고\n",
    "    <img src=\"images/p397 figure 10.12.png\" width = \"65%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Choice of Dissimilarity Measure\n",
    "* Euclidean distance외의 다른 dissimilarity measure 적용가능\n",
    "* *correlation-based distance*의 경우, 비록 두 observation간 Euclidean distance가 멀더라도, observation이 가지고있는 feature들이 서로 강하게 correlated되어 있는지를 판단\n",
    "    - dissimilarity measure의 선택문제는 agglomerative 계열에서의 중요한 판단\n",
    "    - correlation-based distance 사용시, 서로 쇼핑규모의 차이가 큰 두명 소비자 가정시, 서로간 쇼핑선호도 기반 clustering 가능해짐\n",
    "        1. obs#1과 obs#3은 서로 Euclidean distance 가깝다\n",
    "        2. obs#1과 obs#2는 서로 Euclidean distance가 멀지만, correlation-based distance는 가깝다\n",
    "    <img src=\"images/p398 figure 10.13.png\" width = \"65%\"/>\n",
    "* 더불어, dissimilarity measure의 계산전에 variable을 standard deviation = 1이 되도록 scaling해야하는지 여부도 판단필요\n",
    "    - 만약, S.D=1으로 scaling된 후, dissimilarity를 계산한다면 각 variable들은 hierarchical clustering 수행시 동일한 importance를 가지게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###10.3.3 Practical Issues in Clustering\n",
    "####Small Decisions with Big Consequences\n",
    "* Clustering시 결정되어야 할 중요한 사항들\n",
    "    1. 모든 clustering에 있어서\n",
    "        - observation, feature들에 대한 standardization의 수행여부 : centered to have mean zero/ standard deviation = 1\n",
    "    2. hierarchical clustering에 있어서\n",
    "        - (cluster간의 fusing을 위한 distance 판단을 위해) dissimilarity measure의 종류\n",
    "        - linkage의 종류\n",
    "        - dendrogram상의 cutting point\n",
    "    3. *K*-means clustering에 있어서\n",
    "        - cluster의 개수 *K*\n",
    "\n",
    "####Validating the Clusters Obtained\n",
    "* Clustering 수행시, 진정으로 알고싶은 것\n",
    "    1. true subgroups in the data인지\n",
    "    2. 그저 단순히 noise를 clustering한것인지\n",
    "* Cluster *p*-value를 통해 cluster가 더 존재하는지를 판단하는 방안이 있으나 아직 공신력 확보되진 않음\n",
    "\n",
    "####Other Considerations in Clustering\n",
    "* *K*-means와 hierarchical clustering은 각 observation을 특정 cluster에 배정하므로, 때론 적절치 않을 수 있음\n",
    "    - 가령, 대부분의 observation은 clustering이 적합하나, 일부 outlier가 존재하는 경우를 고려시...\n",
    "    - 따라서, Mixture model 통한 접근이 가능하며, soft version of *K*-means 등이 존재\n",
    "\n",
    "####A Tempered Approach to Interpreting the Results of Clustgering\n",
    "* Clustering시 설정하는 각종 중요 parameter에 의해 결과는 크게 상이할 수 있음\n",
    "* 따라서, 다양한 params 설정을 통해 clustering을 수행후, pattern의 consistency를 주의깊게 살펴볼 필요있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
