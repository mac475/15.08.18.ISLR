{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\", \"malgun gothic\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width: 100%;\n",
       "    }\n",
       "    ul {\n",
       "        line-height: 145%;\n",
       "        font-size: 90%;\n",
       "    }\n",
       "    li {\n",
       "        margin-bottom: 0.2em;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top: 12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width: 90%;\n",
       "//        margin-left:auto;\n",
       "//        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mac475의 ipython 표준 style을 적용함\n",
    "from IPython.core.display import HTML\n",
    "styles = open(\"styles/custom.css\", \"r\").read()\n",
    "HTML( styles )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#9. Support Vector Machines\n",
    "* SVM : support vector machine은 classification 방법\n",
    "    - maximal margin classifier의 generalization임 : linear boundary\n",
    "    - SVC : support vector classifier : maximal margin classifier의 다양한 case 적용\n",
    "    - support vector machine : non-linearity를 지원하는 classifier : 단, 2개 class 지원\n",
    "    - over the rainbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##9.1 Maximal Margin Classifier\n",
    "###9.1.1 What Is a Hyperplane?\n",
    "* $p$-dimensional space에서 $hyperplane$은 $p-1$ dimension의 affine subspace임 (아핀공간)\n",
    "    - 2차원 dataset의 경우, hyperplane : $\\beta_0+\\beta_1X_1+\\beta_2X_2 = 0$\n",
    "    - $p$차원 dataset의 경우, hyperplane : $\\beta_0+\\beta_1X_1+\\beta_2X_2+\\cdots+\\beta_pX_p = 0$\n",
    "###9.1.2 Classification Using a Separating Hyperplane\n",
    "* $n \\times p$ data matrix X를 $n$ training observations, $p$-dimensional space라 할때, 다음과 같으며,\n",
    "    $$\n",
    "        x_1 = \n",
    "                \\left(\n",
    "                    \\begin{matrix}\n",
    "                        x_{11} \\\\\n",
    "                        \\vdots \\\\\n",
    "                        x_{1p}\n",
    "                    \\end{matrix}\n",
    "                \\right)\n",
    "        ,\\cdots,\n",
    "        x_n = \n",
    "                \\left(\n",
    "                    \\begin{matrix}\n",
    "                        x_{n1} \\\\\n",
    "                        \\vdots \\\\\n",
    "                        x_{np}\n",
    "                    \\end{matrix}\n",
    "                \\right)                \n",
    "    $$\n",
    "    - 이때, test observation $x^* = (x_1^* \\cdots x_p^*)^T$에 대해, classifier를 만들어 내는 것이 목표\n",
    "        - 참고 : 전치행렬 (transposed matrix) : [https://ko.wikipedia.org/wiki/%EC%A0%84%EC%B9%98%ED%96%89%EB%A0%AC]\n",
    "        - 다음과 같이 다양한 형태로 *separating hyperplane*이 가능\n",
    "        <img src=\"images/p340 figure 9.2.png\" width = \"65%\"/>\n",
    "        - hyperplane은 다음과 같은 형태임\n",
    "            $$\n",
    "                \\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\cdots+\\beta_px_{ip} > 0 \\quad if \\, y_i=1\n",
    "                \\\\\n",
    "                \\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\cdots+\\beta_px_{ip} < 0 \\quad if \\, y_i=-1\n",
    "            $$\n",
    "        - test observation 판단시, $f(x^*) = \\beta_0+\\beta_1x_1^*+\\beta_2x_2^*+\\cdots+\\beta_px_p^*$ 활용, positive/ negative 판단하여 class assign\n",
    "        - 또한, $f(x^*)$값의 0으로부터 떨어진 정도를 통해 *magnitude*를 활용할 수도 있음\n",
    "            - 즉, magnitude가 크다면, hyperplane에서 멀리 떨어져 있으므로, class assign에 대한 confidence 높으며,\n",
    "            - magnitude 작다면, confidence 낮다고 볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###9.1.3 The Maximal Margin Classifier\n",
    "* 각 class를 classify하는 hyperplane은 다양한 형태로 존재가능함\n",
    "    - 따라서, 우리가 사용할 hyperplane을 선택하기 위한 방법이 있어야 함\n",
    "* *maximal margin hyperplane* : 가장 가까운 training observation들과 가장 먼 hyperplane\n",
    "    - 이때, 가장 가까운 training observation들을 *support vector*라고 함\n",
    "    - *maximal margin hyperplane*은 support vector에만 dependency가 있으며, 다른 observation들의 변화와는 무관함\n",
    "    <img src=\"images/p342 figure 9.3.png\" width = \"65%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###9.1.4 Construction of the Maximal Margin Classifier\n",
    "* *maximal margin classifier*는 $x_1, \\cdots, x_n \\in \\mathbb{R^p}$에 있어 maximal margin hyperplane임\n",
    "    - *참고 : \"$x_1, \\cdots, x_n \\in \\mathbb{R^p}$\"의 의미 : $x_1, \\cdots x_n$은 length $p$의 dimension을 가진다*\n",
    "* Solution\n",
    "    $$\n",
    "        maximize_{\\beta_0, \\beta_1,\\cdots,\\beta_p} M\n",
    "        \\\\\n",
    "        subject \\, to \\quad \\sum\\limits_{j=1}^p\\beta_j^2 = 1\n",
    "        \\\\\n",
    "        y_i(\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\cdots+\\beta_px_{ip}) \\ge M \\quad \\forall \\, i = 1,\\cdots,n \\quad M > 0\n",
    "    $$\n",
    "    - *참고 : \"$\\forall$\"*의 의미 : universal quantifier : 정의역에 속하는 모든 값에 대하여 참이다\n",
    "    - *참고 : $\\beta_j^2$의 sum이 1인것은 normalized로 적용했기 때문임\n",
    "    - $M > 0$이므로 모든 observataion은 hyperplane을 중심으로 classified\n",
    "    - $M$은 margin을 의미하며, $M$을 maximize하는 $\\beta_0, \\beta_1, \\cdots, \\beta_p$를 optimization해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###9.1.5 The Non-separable Case\n",
    "* 현실의 많은 경우에 있어 maximal margin classifier hyperplane이 존재하지 않음\n",
    "* 이런경우, *soft margin*를 활용하여 *almost separate the classes*를 수행하는 hyperplane을 만들어 활용\n",
    "* Non-separable case에 대응하는 maximal margin classifier의 확장된 이 일반화를 *support vector classifier*라 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##9.2 Support Vector Classifier\n",
    "###9.2.1 Overview of the Support Vector Classifier\n",
    "* hyperplane으로 observation의 class를 명확히 구분할 수 없는 다수의 경우 존재\n",
    "    <img src=\"images/p344 figure 9.4.png\" width = \"55%\"/>\n",
    "* 또한, 아래와 같이 single observation의 변화로도 extremely한 hyperplane 변화를 발생시킬 수도 있으며,\n",
    "  *margin이 지나치게 tiny하여* confidence가 낮아지기도 함\n",
    "    <img src=\"images/p345 figure 9.5.png\" width = \"60%\"/>\n",
    "* 따라서, 완화된 조건의 hyperplane이 필요하게 되며, 완화의 주된 목적은 다음과 같음\n",
    "    - hyperplane의 robustness\n",
    "    - *most (not all) training observation*에 대한 더 나은 classification\n",
    "* *soft margin classifier*라고도 하는 *support vector classifier* 통해 observation 대한 hyperplane을 다음과 같이 완화\n",
    "    - 아래와 같이, 각 observation에 대해\n",
    "        - fig 9.6의 좌측 1, 8과 같이 margin내 위치함을 허용 (incorrect margin)\n",
    "        - fig 9.6의 우측 11, 12와 같이 hyperplane에서의 잘못된 위치를 허용 (incorrect side of the hyperplane)\n",
    "<img src=\"images/p346 figure 9.6.png\" width = \"75%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###9.2.2 Details of the Support Vector Classifier\n",
    "* Support vector classifier를 통해, test observation을 classify함\n",
    "* 이때, hyperplane은 training observation을 2개의 class로 가장 잘 separate하는 것으로 선택되나, 일부 misclassify를 허용함\n",
    "* 이는 다음의 optimization 절차에 따라 수행됨\n",
    "* Solution\n",
    "    $$\n",
    "        maximize_{\\beta_0, \\beta_1,\\cdots,\\beta_p, \\epsilon_1,\\cdots,\\epsilon_n} M\n",
    "        \\\\\n",
    "        subject \\, to \\quad \\sum\\limits_{j=1}^p\\beta_j^2 = 1\n",
    "        \\\\\n",
    "        y_i(\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\cdots+\\beta_px_{ip}) \\ge M(1-\\epsilon_i), \\quad \\epsilon_i \\ge 0, \\quad, \\sum\\limits_{i=1}^n\\epsilon_i \\le C\n",
    "    $$\n",
    "    - *참고 : $\\beta_j^2$의 sum이 1인것은 normalized로 적용했기 때문임\n",
    "    - *참고 : $C \\ge 0$의 tuning parameter임\n",
    "    - *참고 : maximal margin classifier와 달리 slack variable $\\epsilon$ 존재, 이를 통해 observation이 wrong margin, 혹은 hyperplane에 존재하도록 허용함 (slack : a 느슨한)\n",
    "    - $slack \\, variable \\, \\epsilon$ : 각 observation의 margin/ hyperplane에 대한 상대적 위치를 다음과 같이 알려줌\n",
    "        - $\\epsilon_i = 0$ : correct side of margin\n",
    "        - $\\epsilon_i > 0$ : wrong side of margin\n",
    "        - $\\epsilon_i > 1$ : wrong side of hyperplane\n",
    "    - $tuning \\,\\, parameter \\,\\, C$ : $\\epsilon_i$의 sum임 → 즉, 이는 margin과 hyperplane에 대한 violation 허용수준 (severity)\n",
    "        - 즉, $C$는 $n$ observations에 의해 violate가능한 margin량에 대한 *budget*으로 간주가능함\n",
    "            - $C = 0$ : violation 허용가능 budet 없음\n",
    "            - $C > 0$ : $C$개 미만의 observation이 wrong side of hyperplane 허용됨 ($\\because \\, \\epsilon_i > 1$ : wrong side of hyperplane)\n",
    "        - budet $C$가 증가하면, margin violation 허용이 증가하여 margin이 widen. $C$가 감소하면, margin이 narrows\n",
    "        - 실무적으론, $C$는 cross-validation에 의해 선택, (training observation에 대한 정확성 관점) bias-variance trade-off를 조절\n",
    "            - $C$가 작으면 : narrow margin → rarely violate → low bias, high variance\n",
    "            - $C$가 크다면 : wide margin → allow violate → more bias, lower variance\n",
    "- 위의 수식과, 내용을 고려하면... (즉, $\\epsilon$ 범위따른 violation 허용을 고려시) margin상에 있거나, margin을 violate한 observation만이 hyperplane에 영향을 줌 → 이러한 observations를 *support vectors*라고 함\n",
    "    - tuning parameter $C$가 크면, margin 증가, violate가능한 observation 많아져 → support vectors 많아짐\n",
    "    - 즉, 많은 수의 observation들이 hyperplane 결정에 관여하게 됨\n",
    "    \n",
    "    <img src=\"images/p348 figure 9.7.png\" width = \"70%\"/>\n",
    "    \n",
    "- 위 figure에서,\n",
    "    - 좌상단은 wide margin : high bias, low variance\n",
    "    - 우하단은 narrow margin : low bias, high variance\n",
    "* support vector classifier는\n",
    "    - 전체 observation중에서\n",
    "    - 일부 support vectors에 의해서만 hyperplane이 결정되므로\n",
    "    - quite robust하다고 할 수 있음\n",
    "* 이는 LDA와 비교시, estimate에 대한 LDA classifier는\n",
    "    $$\\hat{\\delta_k}(x) = x \\cdot  \\frac{\\hat{\\mu_k}}{\\hat{\\sigma}^2} - \\frac{\\hat{\\mu_k}^2}{2\\hat{\\sigma}^2} + log(\\hat{\\pi}_k)$$와 같이 표현되는 것을 상기시, $k$ class의 평균인 $\\hat{\\mu}_k$가 계산되는 점을 보면,\n",
    "    - LDA 계열의 classification은 각 class에 포함된 'all' observation의 mean값에 의존하며\n",
    "    - class에 대한 covariance에 대한 사전조건이 전제된다는 측면에서\n",
    "    - support vector 계열과 차이를 보임\n",
    "    * 참고 : 단, logistic regression 경우, LDA와 달리, boundary에서 멀리 떨어진 observation 대한 sensityvity 매우 낮다\n",
    "    * 하여튼, Chap.4를 다시 찾아볼 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##9.3 Support Vector Machines\n",
    "* Support verctor classifier는 2-class에 대한 boundary가 linear한 경우의 접근방법임\n",
    "    - 하지만, 실제로는 boundary가 non-linear한 경우가 빈번함\n",
    "    - 이런경우는, linear regression에서도 접할 수 있었으며, 우리는 feature space를 enlarge함으로써 극복했었음\n",
    "        - 즉, predictors에 대한 quadratic, cubic(2차, 3차의) 등 polynominal 함수의 도입. 즉\n",
    "        $$X_1, X_2, \\cdots , X_p \\,\\, → \\,\\, X_1, X_1^2, X_2, X_2^2, \\cdots, X_p, X_p^2$$\n",
    "* 이는 다음의 optimization 절차에 따라 수행됨\n",
    "* Solution\n",
    "    $$\n",
    "        maximize_{\\beta_0, \\beta_{11}, \\beta_{12},\\cdots,\\beta_{p1}, \\beta_{p2}, \\epsilon_1,\\cdots,\\epsilon_n} M\n",
    "        \\\\\n",
    "        subject \\, to \\quad y_i(\\beta_0 + \\sum\\limits_{j=1}^p\\beta_{j1}x_{ij} + \\sum\\limits_{j=1}^p\\beta_{j2}x_{ij}^2) \\ge M(1-\\epsilon_i)\n",
    "        \\\\\n",
    "        \\quad \\epsilon_i \\ge 0, \\quad, \\sum\\limits_{i=1}^n\\epsilon_i \\le C, \\quad \\sum\\limits_{j=1}^p\\sum\\limits_{k=1}^2\\beta_{jk}^2 = 1\n",
    "    $$\n",
    "    - *참고 : 위의 solution에서 $y_i(\\beta_0 + \\sum\\limits_{j=1}^p\\beta_{j1}x_{ij})$는 support vector classifier와 동일\n",
    "    - *참고 : $\\beta_{jk}^2$의 sum이 1인것은 normalized로 적용했기 때문임\n",
    "    - 이때, non-linearity를 위한 방법은 polynominal ($X, X^2, X^3, \\cdots, X^n$), 혹은 interaction ($X_1 \\cdot X_2$) 등 다양하게 가능\n",
    "    - 하지만, unmanageable한 것 극복위해 효과적 방법의 모색이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###9.3.2 The Support\n",
    "* Support vector machines(SVM)은 support vector classifier의 extension임\n",
    "    - 이때, extension이란, *kernels*를 활용하여 feature space를 확장한 결과의 의미임\n",
    "    - 기본적으로는, class간의 non-linear한 boundary를 accommodate(수용)하기위한 목적임\n",
    "* Support vector classifier의 경우, observation들의 *inner product*만 involve됨\n",
    "    - 이때, 2개의 *r*-vector a와 b의 inner product는 $\\langle a,b \\rangle = \\sum_{i=1}^ra_i\\cdot b_i$와 같이 정의됨\n",
    "    - *참고 : 벡터의 내적 (inner product)\n",
    "        - 링크1[http://j1w2k3.tistory.com/627], 링크2[http://tip.daum.net/question/54855462]\n",
    "        - $\\vec{a}, \\vec{b}$가 이루는 각이 $\\theta$일 때, 내적 inner product $\\langle a, b\\rangle = \\vec{a} \\cdot \\vec{b} = |\\vec{a}||\\vec{b}|cos\\theta$이며, 이는 scalar값임\n",
    "    - *참고 : 삼각함수의 그래프\n",
    "    <img src=\"images/p351 trigonometrical graph.png\" width = \"60%\"/>\n",
    "- 이때, 2개의 observation $x_i, x_{i\\prime}$의 inner product는 다음과 같으므로,\n",
    "    $$\\langle x_i, x_{i\\prime}\\rangle = \\sum\\limits_{j=1}^px_{ij}x_{i\\prime j}$$\n",
    "- linear support vector classifier는 다음과 같이 표현됨\n",
    "    $$f(x) = \\beta_0 + \\sum\\limits_{i=1}^p\\alpha_i \\langle x, x_i\\rangle$$\n",
    "    - 이때, 1개의 training observation당 *n*개의 parameter가 존재 ($\\alpha_i, \\, i = 1,\\cdots,n$)\n",
    "- $\\alpha_1, \\cdots, \\alpha_n$과 $\\beta_0$의 estimation 위해선 모든 training observation 조합간 $\\binom{n}{2}$ inner product $\\langle x_i, x_{i \\prime} \\rangle$가 필요\n",
    "    - 단, observation이 support vector인 경우에만 $\\alpha_i \\neq 0 $ (따라서, observation이 support vector가 아니라면 $\\therefore \\alpha_i = 0$)\n",
    "    - *S*가 support vector 구성하는 점들의 집합이라면, 다음으로, 위 정리한 계산보다 훨씬 적은량의 계산이 소요된다\n",
    "        $$f(x) = \\beta_0 + \\sum\\limits_{i \\in S}\\alpha_i\\langle x, x_i \\rangle$$\n",
    "    - 요약하면, linear classifier $f(x)$에 있어 coefficient의 계산시 *inner product*를 통해 수행됨\n",
    "* 앞으로 *inner product*의 generalization된 형태로써 다음과 같이 표현하며,\n",
    "    $$K ( x_i, x_{i\\prime} )$$\n",
    "    - 이때, K를 *kernel*로 부름 : kernel은 2개 observation간의 similarity를 수량화한 것임\n",
    "    - 다음은 *linear kernel*\n",
    "        $$K ( x_i, x_{i\\prime} ) = \\sum\\limits_{j=1}^px_{ij}x_{i\\prime j}$$\n",
    "    - 다음은 *polynominal kernel*이며, non-linear한 kernel임\n",
    "        $$K ( x_i, x_{i\\prime} ) = ( 1 + \\sum\\limits_{j=1}^px_{ij}x_{i\\prime j} )^d$$\n",
    "    - 이렇게 non-linear kernel이 적용된 support vector classifier를 **support vector machine**이라고 함\n",
    "        $$f(x) = \\beta_0 + \\sum\\limits_{i \\in S}\\alpha_i K ( x, x_i ) $$\n",
    "    - 다음은 *radial kernel*이며, radial 형태의 kernel임\n",
    "        $$K ( x_i, x_{i\\prime} ) = exp \\left( -\\gamma \\sum\\limits_{j=1}^p (x_{ij} - x_{i\\prime j})^2\\right)$$\n",
    "    - 아래 좌측이 polynominal kernel, 우측이 radial kerner임\n",
    "        <img src=\"images/p353 figure 9.9.png\" width = \"70%\"/>\n",
    "    - radial kernel의 경우, *local* behavior를 보여줌 → test observation에 nearby한 training observation만 영향력이 있음\n",
    "        * $\\gamma > 0$\n",
    "        1. $\\because$ test observation $x^*$가 training observation $x_i$와 상당히 멀다면,\n",
    "        2. $\\sum_{j=1}^p(x_j^* - x_{ij})^2$가 커질것이며,\n",
    "        3. 따라서, $K(x^*,x_i = ) = exp \\left( -\\gamma \\sum\\limits_{j=1}^p (x_{ij} - x_{i\\prime j})^2\\right)$가 매우 작아지므로\n",
    "        4. training observation $x_i$가 test observation $x^*$가 실질적으로 거의 영향력이 없다는 의미임\n",
    "* Kernel 사용의 이점\n",
    "    - non-linear한 대상에 대한 간단한 계산방식임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###9.3.3 An Application to the Heart Disease Data\n",
    "* 아래의 LDA VS SVC, SVC VS radial kernel 참고\n",
    "    - training observation 대상으로, SVC가 LDA대비 우수, $\\gamma$가 커질수록, non-linear 특성반영되므로 ROC Curve가 우수\n",
    "    <img src=\"images/p354 figure 9.10.png\" width = \"70%\"/>\n",
    "    - 하지만, test observation 대상으로는 반드시 위의 결과와 같지 않음\n",
    "    <img src=\"images/p355 figure 9.11.png\" width = \"70%\"/>\n",
    "    - 요약 : training data에 대한 overfitting을 주의할 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##9.4 SVMs with More than Two Classes\n",
    "* 2개 이상 class에 대한 SVM은 다음의 두가지 방법으로 수행\n",
    "    - *one-versus-one*\n",
    "    - *one-versus-all*\n",
    "\n",
    "###9.4.1 One-Versus-One Classification\n",
    "    - 그런 것이 있음 : 생략\n",
    "\n",
    "###9.4.2 One-Versus-All Classification\n",
    "    - 그런 것이 있음 : 생략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##9.5 Relationship to Logistic Regression\n",
    "* SVC의 classifier $f(X) = \\beta_0 + \\beta_1X_1 + \\cdots + \\beta_pX_p$는 다음과 같이 rewrite 표현가능\n",
    "    $$minimize_{\\beta_0, \\beta_1, \\cdots, \\beta_p} \\{ \\sum\\limits_{j=1}^n max[0,1-y_if(x_i)] + \\lambda \\sum\\limits_{j=1}^p \\beta_j^2 \\}$$\n",
    "    - $\\lambda > 0$인 tuning parameter임\n",
    "    - 이때, $\\lambda$가 크면, $\\beta_1 \\cdots \\beta_p$는 작아지고, margin 허용범위가 커진다고 함. 이는 아래를 보면,\n",
    "        $$y_i(\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\cdots+\\beta_px_{ip}) \\ge M(1-\\epsilon_i), \\quad \\epsilon_i \\ge 0, \\quad, \\sum\\limits_{i=1}^n\\epsilon_i \\le C$$\n",
    "        - $\\beta$ 계열 감소에 따라, 좌항이 감소하며, 따라서, 우항도 감소하게 되는데, 즉, $M(1-\\epsilon_i)$가 작아지려면, $\\epsilon_i$가 커져야 함\n",
    "        - 즉, $\\epsilon_i$의 증가는 결과적으로 tuning parameter $C$의 증가를 의미하는데, 앞에서 정리되었던 내용 반복하면,\n",
    "            - $C$가 작으면 : narrow margin → rarely violate → low bias, high variance\n",
    "            - $C$가 크다면 : wide margin → allow violate → more bias, lower variance\n",
    "    - 요약하면, $\\lambda$와 tuning parameter $C$는 동일 경향을 가지게 됨\n",
    "        - $\\lambda, or \\, C$가 작으면 : narrow margin → rarely violate → low bias, high variance\n",
    "        - $\\lambda, or \\, C$가 크다면 : wide margin → allow violate → more bias, lower variance\n",
    "    - 이때, $\\lambda\\sum_{j=1}^p\\beta_j^2$은 ridge regression에서의 *ridge penalty*와 같은 역할을 하며, bias-variance trade-off를 조절함\n",
    "    - 참고* : ridge penalty : Chap.6\n",
    "        - Ridge regression이 least squares 대비 우수한 점은 *bias-variance trade-off에 기인*\n",
    "        - $\\lambda$ 증가시 flexibility 감소하여, bias는 증가하나 variance는 감소함 (bias : black, variance : green, MSE : purple)    \n",
    "* 하여튼, 위의 rewrite 수식은 기본적으로 다음과 같은 **Loss + Penalty** 형식임\n",
    "    $$minimize_{\\beta_0, \\beta_1, \\cdots, \\beta_p} \\{ L(X,y,\\beta) + \\lambda P(\\beta) \\}, \\quad \\lambda \\ge 0$$\n",
    "* 참고로, ridge/ lasso regression의 Loss는 다음과 같은 형태로 정리됨\n",
    "    $$L(X, y, \\beta) = \\sum\\limits_{i=1}^n \\left( y_i-\\beta_0-\\sum\\limits_{j=1}^px_{ij}\\beta_j\\right)^2$$\n",
    "    - 이때 ridge regression의 penalty : $P(\\beta) = \\sum_{j=1}^p\\beta_j^2$\n",
    "    - lasso regression의 penalty : $P(\\beta) = \\sum_{j=1}^p|\\beta_j|$와 같이 정리되며,\n",
    "    - SVC의 Loss는\n",
    "        $$L(X,y,\\beta) = \\sum\\limits_{i=1}^n max[0,1 - y_i(\\beta_0 + \\beta_1x_{i1} + \\cdots + \\beta_px_{ip})]$$\n",
    "        와 같이 정리되며, 이를 *hinge loss*라 함\n",
    "* 아래를 보면, $y_i(\\beta_0+\\cdots+\\beta_px_{ip})$와 Loss간의 관계임\n",
    "    - 이때, $y_i(\\beta_0+\\cdots+\\beta_px_{ip}) \\ge 1$ 즉, $\\epsilon_i = 0$이므로, correct side of margin의 경우와 같이\n",
    "    - well separated된 경우는, SVM이 logistic regression보다 좋음 (Loss가 작다)\n",
    "    - poor separated된 경우는, logistic regression이 SVM보다 좋음\n",
    "    - 참고* : loss란 실제값과 계산값간의 차이를 의미\n",
    "    <img src=\"images/p358 figure 9.12.png\" width = \"70%\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
